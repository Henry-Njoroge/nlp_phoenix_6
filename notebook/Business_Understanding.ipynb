{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2982a43e",
   "metadata": {},
   "source": [
    "# TWIITER SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd16698-5d10-4e33-9d2b-aca789da5082",
   "metadata": {},
   "source": [
    "## Business Understanding.\n",
    "#### Business Overview\n",
    "\n",
    "In today’s digital age, social media platforms such as Twitter have become powerful spaces where consumers freely express their opinions about brands, products, and user experiences.\n",
    "\n",
    "For technology giants like Apple and Google, tweets represent an unfiltered stream of public perception that can influence reputation, marketing strategies, and customer loyalty. The abundance of this user-generated content provides a valuable opportunity for organizations to leverage **Natural Language Processing (NLP)** techniques to uncover actionable insights from unstructured text data.\n",
    "\n",
    "This project utilizes a labeled dataset of tweets sourced from **CrowdFlower (via data.world)**, containing human-rated sentiments toward Apple and Google products. By analyzing this data, the project aims to build an automated system capable of classifying tweet sentiments and essential step toward understanding customer emotions and brand perception.\n",
    "\n",
    "#### Business Objectives\n",
    "\n",
    "The overall business objective of this project is to build an automated system that can analyze and classify sentiments expressed in tweets related to Apple and Google products. The system will provide insights into how consumers perceive these brands and their respective products, allowing decision-makers to understand market sentiment trends and react accordingly.\n",
    "\n",
    "Specifically, the project aims to:\n",
    "\n",
    "* Identify and categorize public sentiments toward Apple and Google products as positive, negative, or neutral.\n",
    "\n",
    "* Develop an NLP model that demonstrates the feasibility of automated sentiment analysis.\n",
    "\n",
    "* Provide data-driven insights that can guide brand management, product improvement and customer engagement strategies.\n",
    "\n",
    "* Enable future scalability where the approach can be extended to other brands or social media platforms.\n",
    "\n",
    "#### Business Problem\n",
    "\n",
    "Organizations such as Apple and Google receive continuous feedback from millions of users daily on social media. Manually analyzing this information to identify consumer attitudes is inefficient and impractical.\n",
    "\n",
    "Tweets often contain informal language, slang and abbreviations, making traditional text analysis approaches insufficient.\n",
    "Without an automated solution, it becomes challenging for companies to:\n",
    "\n",
    "* Detect sudden shifts in customer sentiment,\n",
    "\n",
    "* Identify negative feedback early enough to take corrective action, and\n",
    "\n",
    "* Understand product-related discussions that could inform business strategy.\n",
    "\n",
    "This project addresses the need for an automated sentiment classification model that can efficiently process text data and provide reliable, timely insights into customer sentiment.\n",
    "\n",
    "#### Success Criteria\n",
    "\n",
    "Achieve acceptable model performance metrics (≥ 80% accuracy or balanced F1-score for binary classification).\n",
    "\n",
    "Produce interpretable and reproducible results.\n",
    "\n",
    "Ensure a well-documented and modular workflow following the CRISP-DM process: Business Understanding → Data Understanding → Data Preparation → Modeling → Evaluation → Deployment.\n",
    "\n",
    "Deliver clear visualizations and concise insights to support decision-making\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfafb5c-efea-4594-9ddf-f8dd91024e2f",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a56de91",
   "metadata": {},
   "source": [
    "\n",
    "This dataset contains Twitter/X posts (tweets) from the SXSW conference with sentiment analysis labels. The data has 3 columns:\n",
    "1. tweet_text: The original tweet content mentioning tech products and SXSW experiences\n",
    "2. emotion_in_tweet_is_directed_at (Brand_Product): The specific Apple or Google product mentioned \n",
    "3. is_there_an_emotion_directed_at_a_brand_or_product (Emotion): The sentiment expressed\n",
    "    \n",
    "The tweets discuss various Apple and Google products with users sharing their experiences, complaints, and excitement during the tech conference. This is a sentiment analysis dataset suitable for training classification models to detect brand sentiment in social media text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cde70cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6914c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp38-cp38-win_amd64.whl (300 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from wordcloud) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from wordcloud) (1.24.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from wordcloud) (8.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (2024.8.30)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (3.1.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153f827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: wordcloud in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (1.9.4)\n",
      "Collecting pillow\n",
      "  Downloading pillow-10.4.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from wordcloud) (3.3.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.6.1 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from wordcloud) (1.24.4)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2020.06.20 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (2024.8.30)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->wordcloud) (3.1.4)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Installing collected packages: pillow\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 8.0.0\n",
      "    Uninstalling Pillow-8.0.0:\n",
      "      Successfully uninstalled Pillow-8.0.0\n",
      "Successfully installed pillow-10.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "fbprophet 0.7.1 requires cmdstanpy==0.9.5, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade wordcloud pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d3abc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pillow in c:\\users\\user\\anaconda3\\envs\\learn-env\\lib\\site-packages (10.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6173b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the libraries\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_df = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding='latin1')\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af2e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027aaa83",
   "metadata": {},
   "source": [
    "🔹 Observations\n",
    "\n",
    "Dataset loaded successfully with 3 main columns.\n",
    "\n",
    "Data represents tweets related to Apple and Google products.\n",
    "\n",
    "Encoding changed to 'latin1' to handle special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06bb4e5",
   "metadata": {},
   "source": [
    "We have 9093 observations and 3 Variables\n",
    " - The 3 variables have 'string' as a datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape\n",
    "print(f\"Dataset contains {data_df.shape[0]} rows and {data_df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206eaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the columns\n",
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with two null columns\n",
    "data_df = data_df.dropna(thresh=data_df.shape[1] - 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138346bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfdb3b4",
   "metadata": {},
   "source": [
    "Only 1 row was dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcc1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking duplicates\n",
    "len(data_df[data_df.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a127949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "data_df.drop_duplicates(keep = 'first', inplace = True)\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c4c28",
   "metadata": {},
   "source": [
    "There were 22 duplicates which were dropped, therefore giving us 9070 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename to simple names for easy use\n",
    "df = data_df.rename(columns={\n",
    "    data_df.columns[0]: 'tweet',\n",
    "    data_df.columns[1]: 'product',\n",
    "    data_df.columns[2]: 'sentiment'\n",
    "})\n",
    "\n",
    "# Check first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c842f",
   "metadata": {},
   "source": [
    "# Tweet Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c11a93",
   "metadata": {},
   "source": [
    "#### 1.Remove Twitter handles and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy\n",
    "df['cleaned_tweet'] = (df['tweet'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove handles and strip the '#' from hashtags, keep the hashtag words\n",
    "df['cleaned_tweet'] = (df['cleaned_tweet'].astype(str)\n",
    "                                .str.replace(r'@\\w+', '', regex=True)   \n",
    "                                .str.replace(r'#', '', regex=True)      \n",
    "                                .str.strip())\n",
    "\n",
    "\n",
    "df[['tweet', 'cleaned_tweet']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab040032",
   "metadata": {},
   "source": [
    "#### 2. Remove URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f91afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove URLs\n",
    "df['cleaned_tweet'] = (\n",
    "    df['cleaned_tweet']\n",
    "      .str.replace(r'http\\S+|www\\S+', '', regex=True)  \n",
    "      .str.replace(r'\\s+', ' ', regex=True)            \n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "# Show result so we can compare\n",
    "df[['tweet', 'cleaned_tweet']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2946acce",
   "metadata": {},
   "source": [
    "#### 3.Remove punctuations, numbers and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1937c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove special characters and numbers (but keep ! and ?)\n",
    "df['cleaned_tweet'] = (\n",
    "    df['cleaned_tweet']\n",
    "      .str.replace(r'[^a-zA-Z ]', '', regex=True)   \n",
    "      .str.replace(r'\\s+', ' ', regex=True)           \n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "df[['tweet', 'cleaned_tweet']].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a960035",
   "metadata": {},
   "source": [
    "#### 4. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d679a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and lowercasing\n",
    "#df['cleaned_tweet'] = df['cleaned_tweet'].apply(lambda x: word_tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e51a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tweet'] = df['cleaned_tweet'].apply(\n",
    "    lambda x: word_tokenize(x.lower()) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79753f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_tweet'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e031ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reduce elongation (e.g., \"soooo\" → \"soo\")\n",
    "def reduce_elongation(word):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "\n",
    "# Apply elongation reduction to each token\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].apply(lambda tokens: [reduce_elongation(w) for w in tokens])\n",
    "\n",
    "# Preview results\n",
    "df[['tweet', 'cleaned_tweet']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e86b9",
   "metadata": {},
   "source": [
    "#### 6. Remove single characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba189d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove single characters\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].replace(re.compile(r\"(^| ).( |$)\"), \" \")\n",
    "# Check result\n",
    "df[['tweet', 'cleaned_tweet']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008ec7c",
   "metadata": {},
   "source": [
    "#### 7. Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0658047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define stopwords and exceptions\n",
    "stop_words = set(stopwords.words('english'))\n",
    "negation_words = {\"no\", \"not\", \"nor\", \"never\"}\n",
    "custom_stopwords = stop_words - negation_words  \n",
    "\n",
    "# Remove stopwords directly from tokenized lists\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].apply(\n",
    "    lambda tokens: [word for word in tokens if word not in custom_stopwords]\n",
    ")\n",
    "\n",
    "# Check result\n",
    "df[['tweet', 'cleaned_tweet']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b99f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check top 20 most frequent words\n",
    "word_count = Counter()\n",
    "\n",
    "for tokens in df['cleaned_tweet']:\n",
    "    for word in tokens:\n",
    "        word_count[word] += 1\n",
    "\n",
    "# Display top 20 most frequent words\n",
    "word_count.most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c345eaf6",
   "metadata": {},
   "source": [
    "#### Removing reject words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reject words that add no sentiment meaning\n",
    "reject_words = { 'link', 'sxsw', }\n",
    "\n",
    "# Apply removal to your tokenized column\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].apply(\n",
    "    lambda tokens: [w for w in tokens if w not in reject_words]\n",
    ")\n",
    "# Preview\n",
    "df[['tweet', 'cleaned_tweet']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d7f703",
   "metadata": {},
   "source": [
    "#### 8.Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Helper to map POS tags to WordNet format\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun\n",
    "\n",
    "# Apply POS tagging + lemmatization\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].apply(\n",
    "    lambda tokens: [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tag(tokens)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check results\n",
    "df[['tweet', 'cleaned_tweet']].sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be79699e",
   "metadata": {},
   "source": [
    "#### 9. Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Initialize spellchecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function to correct spelling for tokenized data\n",
    "def correct_spelling(tokens):\n",
    "    # Find misspelled words in the token list\n",
    "    misspelled = spell.unknown(tokens)\n",
    "\n",
    "    corrected_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in misspelled:\n",
    "            corrected_tokens.append(spell.correction(word))  # replace with corrected word\n",
    "        else:\n",
    "            corrected_tokens.append(word)  # keep as is\n",
    "\n",
    "    return corrected_tokens\n",
    "\n",
    "# Apply to your dataframe (tokenized column)\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].apply(correct_spelling)\n",
    "\n",
    "# Preview\n",
    "df[['tweet', 'cleaned_tweet_text']].head(10)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e81fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join back into text for TF-IDF and model training\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].apply(lambda tokens: ' '.join(tokens))\n",
    "df[['tweet', 'cleaned_tweet']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773de67",
   "metadata": {},
   "source": [
    "Overall Observations on the Tweet Column.\n",
    "\n",
    "    - All tweets were successfully cleaned and normalized, removing URLs, mentions, hashtags, numbers, and emojis.\n",
    "\n",
    "    - Text was converted to lowercase for consistency across the dataset.\n",
    "\n",
    "    - Contractions like “don’t” → “do not” were expanded to preserve meaning.\n",
    "\n",
    "    - Extra spaces and punctuation were removed to simplify token patterns.\n",
    "\n",
    "    - Removed single character words like \"I\", \"g\"\n",
    "\n",
    "    - Applied tokenization, stopword removal, and lemmatization — reducing words to their root forms.\n",
    "    \n",
    "    - Some tweets became shorter due to removal of filler or redundant words, but key sentiment-carrying terms remain intact.\n",
    "    \n",
    "    - Ran a spell checker through the words to correct any spelling errors. \n",
    "\n",
    "    - The resulting text is now noise-free and uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6561b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " #Combine all tweets\n",
    "all_words = ' '.join(df['cleaned_tweet'])\n",
    "word_freq = Counter(all_words.split())\n",
    "\n",
    "# WordCloud\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(WordCloud(width=800, height=400, background_color='white').generate(all_words))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beeb417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flatten the token lists into one long string\n",
    "all_words = ' '.join([' '.join(tokens) for tokens in df['cleaned_tweet']])\n",
    "\n",
    "# Optional: get frequency counts\n",
    "word_freq = Counter(all_words.split())\n",
    "\n",
    "# Generate the WordCloud\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(\n",
    "    WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white'\n",
    "    ).generate(all_words)\n",
    ")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871d4506",
   "metadata": {},
   "source": [
    "Observations from Word Cloud\n",
    "Most Dominant Terms:\n",
    "\n",
    "\"sxsw\" and \"link\" are overwhelmingly the most frequent words, indicating heavy use of the conference hashtag and URL sharing\n",
    "\"ipad\" is the most mentioned product, appearing larger than \"iphone,\" suggesting it was a hot topic (likely due to iPad 2 launch timing)\n",
    "\"google\" and \"apple\" are both prominently featured, confirming the Apple vs. Google product focus\n",
    "\n",
    "Context & Activity Words:\n",
    "\n",
    "\"store,\" \"popup,\" \"launch,\" \"opening\" suggest discussion about physical retail events and product launches at SXSW\n",
    "\"social,\" \"network,\" \"app\" reflect the social media and app-centric nature of conversations\n",
    "\"austin\" appears frequently as the conference location\n",
    "\n",
    "Communication Patterns:\n",
    "\n",
    "\"rt\" (retweet) indicates significant content sharing and viral discussions\n",
    "The prevalence of \"link\" suggests users were sharing articles, apps, and resources rather than just opinions\n",
    "\n",
    "This word cloud confirms the dataset captures tech product buzz during a major industry conference, with heavy emphasis on Apple products and social sharing behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea5ae9",
   "metadata": {},
   "source": [
    "# Product Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051d621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['product'].value_counts(dropna=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20461839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename iPad or iPhone App to app and Other Apple product or service \n",
    "valuechange_map = {\n",
    "    'iPad or iPhone App': 'app',\n",
    "    'Other Apple product or service': 'apple',\n",
    "    'Other Google product or google': 'google'\n",
    "\n",
    "}\n",
    "\n",
    "# Create a readable sentiment column\n",
    "df['product'] = df['product'].map(valuechange_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['product'] = df['product'].fillna('no_data')\n",
    "def brand(row):\n",
    "    \"\"\"\n",
    "    Categorizes or updates the 'brand_updated' column based on keywords in the 'Tweet' column.\n",
    "\n",
    "    Parameters:\n",
    "    - row (pd.Series): A row of a Pandas DataFrame representing a tweet.\n",
    "\n",
    "    Returns:\n",
    "    - str: Updated brand category ('app', 'ipad', 'iphone', 'apple', 'google', 'android', 'pixel', 'playstore')\n",
    "           or the original 'Brand_Product' value.\n",
    "    \"\"\"\n",
    "    tweet = row['tweet'].lower()  # make it case-insensitive\n",
    "\n",
    "    # Apple-related keywords\n",
    "    if 'ipad' in tweet and 'app' in tweet:\n",
    "        return 'app'\n",
    "    elif 'iphone' in tweet and 'app' in tweet:\n",
    "        return 'app'\n",
    "    elif 'itunes' in tweet:\n",
    "        return 'app'\n",
    "    elif 'ipad' in tweet:\n",
    "        return 'ipad'\n",
    "    elif 'iphone' in tweet:\n",
    "        return 'iphone'\n",
    "    elif 'apple' in tweet:\n",
    "        return 'apple'\n",
    "\n",
    "    # Google-related keywords\n",
    "    elif 'google' in tweet:\n",
    "        return 'google'\n",
    "    elif 'android' in tweet:\n",
    "        return 'android'\n",
    "    elif 'pixel' in tweet:\n",
    "        return 'pixel'    \n",
    "    elif 'playstore' in tweet or 'play store' in tweet:\n",
    "        return 'playstore'\n",
    "\n",
    "    # If no match found\n",
    "    else:\n",
    "        return row['product']\n",
    "# Applying the brand function to create a new 'brand_updated' column\n",
    "df['product_updated'] = df.apply(brand, axis=1)\n",
    "df['product_updated'] = df['product_updated'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95109e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"product_updated\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ff968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['product_updated'] != 'no_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2849fa8",
   "metadata": {},
   "source": [
    "Observation\n",
    "\n",
    "In this step, any missing values in the Product column were filled with the word “no_data” so that there are no blank spots in the data.\n",
    "\n",
    "Next, a function called brand() was created to look at each tweet and figure out which brand it’s talking about. \n",
    "The function searches for certain keywords such as “ipad,” “iphone,” “apple,” “google,” “android,” “pixel,” and “playstore.”\n",
    "\n",
    "Based on what it finds, the tweet is labeled with the right brand name. If no matching word is found, the original value in Brand_Product is kept.\n",
    "\n",
    "After running this function, a new column called product_updated was added to the data. \n",
    "This helped organize the brand information better and reduced the number of rows marked as “no_data.”\n",
    "In this step, any missing values in the product column were filled with the word “no_data” so that there are no blank spots in the data.\n",
    "\n",
    "Next, a function called brand() was created to look at each tweet and figure out which brand it’s talking about. \n",
    "\n",
    "The function searches for certain keywords such as “ipad,” “iphone,” “apple,” “google,” “android,” “pixel,” and “playstore.” Based on what it finds, the tweet is labeled with the right brand name. If no matching word is found, the original value in product is kept.\n",
    "\n",
    "After running this function, a new column called product_updated was added to the data. This helped organize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac95fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcfbc96a",
   "metadata": {},
   "source": [
    "# Sentiment Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the sentiment value counts\n",
    "df['sentiment'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean and standardize\n",
    "df['sentiment_cleaned'] = (\n",
    "    df['sentiment']\n",
    "      .astype(str)\n",
    "      .str.lower()\n",
    "      .str.strip()\n",
    "      .replace({\n",
    "          'positive emotion': 'positive',\n",
    "          'negative emotion': 'negative',\n",
    "          \"i can't tell\": 'neutral',   \n",
    "          'no emotion toward brand or product': 'neutral',\n",
    "          'nan': 'neutral'\n",
    "      })\n",
    ")\n",
    "\n",
    "\n",
    "df[['sentiment', 'sentiment_cleaned']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81943d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recheck value counts\n",
    "df['sentiment_cleaned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666bc181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map the value counts\n",
    "df['sentiment_label'] = df['sentiment_cleaned'].map({'positive': 1,\n",
    "                                                     'negative': 0,\n",
    "                                                     'neutral':2 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b649e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the values of the cleaned sentiments\n",
    "df[['sentiment_cleaned', 'sentiment_label']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74875ad3",
   "metadata": {},
   "source": [
    "Observation\n",
    "\n",
    "The sentiment column was successfully standardized and cleaned for consistency.\n",
    "\n",
    "All text values were converted to lowercase and stripped of extra spaces.\n",
    "\n",
    "Original long-form labels (e.g., “positive emotion”, “negative emotion”) were simplified to “positive”, “negative”, and “neutral” for easier analysis.\n",
    "\n",
    "Ambiguous categories such as “I can’t tell” and “no emotion toward brand or product” were logically grouped under neutral.\n",
    "\n",
    "Created a numerical mapping:\n",
    "\n",
    "1 → positive\n",
    "\n",
    "0 → negative\n",
    "\n",
    "2 → neutral\n",
    "\n",
    "This numerical encoding prepares the data for model training and allows both binary and multiclass sentiment classification later.\n",
    "\n",
    "The cleaned sentiment distribution confirms balanced representation across sentiment categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019128ab",
   "metadata": {},
   "source": [
    "# Merged all cleaned columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa6cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only necessary cleaned columns\n",
    "dataset = df[['cleaned_tweet', 'product_updated', 'sentiment_label']].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "dataset = dataset.rename(columns={\n",
    "    'cleaned_tweet': 'clean_tweet',\n",
    "    'product_cleaned': 'brand',\n",
    "    'sentiment_label': 'clean_sentiment'\n",
    "})\n",
    "\n",
    "# Preview the cleaned datase\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c76f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "dataset.duplicated().sum()\n",
    "dataset = dataset.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889587ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned dataset\n",
    "\n",
    "dataset.to_csv('cleaned_twitter_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb32cb4d",
   "metadata": {},
   "source": [
    "# EDA\n",
    "### Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse map the numeric labels back to words\n",
    "sentiment_map = {\n",
    "    1: 'Positive',\n",
    "    0: 'Negative',\n",
    "    2: 'Neutral'\n",
    "}\n",
    "\n",
    "# Create a readable sentiment column\n",
    "dataset['sentiment_text'] = dataset['clean_sentiment'].map(sentiment_map)\n",
    "\n",
    "# Check if mapping worked\n",
    "dataset[['clean_tweet', 'sentiment_text']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badfa850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment distribution as a donut (pie) chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Extract sentiment counts and labels\n",
    "sentiment_counts = dataset['sentiment_text'].value_counts()\n",
    "labels = sentiment_counts.index\n",
    "colors = sns.color_palette('muted')\n",
    "\n",
    "# Plot donut-style pie chart\n",
    "plt.pie(\n",
    "    sentiment_counts,\n",
    "    labels=labels,\n",
    "    autopct=\"%.2f%%\",\n",
    "    startangle=90,\n",
    "    wedgeprops=dict(width=0.5),\n",
    "    colors=colors\n",
    ")\n",
    "\n",
    "plt.title(\"Sentiment Distribution of Tweets\", fontsize=16, fontweight='bold')\n",
    "plt.legend(labels=labels, title=\"Sentiment\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be274f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot brands frequency in ascending order\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "dataset['product_updated'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Mentioned Brands/Products in ascending order\")\n",
    "plt.xlabel(\"Product / Brand\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39ffbe",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The top most tweeted brand is google followed by app then ipad while the least mentioned is other google products or service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Split dataset by sentiment\n",
    "sentiments = dataset['sentiment_text'].unique()\n",
    "\n",
    "# Set up subplots for each sentiment\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for i, sentiment in enumerate(sentiments, 1):\n",
    "    plt.subplot(3, 1, i)\n",
    "    \n",
    "    # Filter tweets for this sentiment\n",
    "    words = ' '.join(dataset[dataset['sentiment_text'] == sentiment]['clean_tweet'])\n",
    "    word_freq = Counter(words.split()).most_common(15)  # top 15 frequent words\n",
    "    \n",
    "    # Convert to DataFrame for easy plotting\n",
    "    freq_df = pd.DataFrame(word_freq, columns=['word', 'count'])\n",
    "    \n",
    "    # Barplot\n",
    "    sns.barplot(x='count', y='word', data=freq_df, palette='muted')\n",
    "    plt.title(f\"Top Words in {sentiment.capitalize()} Tweets\", fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Word\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e00c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bigram vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english')\n",
    "bigrams = vectorizer.fit_transform(dataset['clean_tweet'])\n",
    "\n",
    "# Sum up bigram occurrences\n",
    "bigram_sum = bigrams.sum(axis=0)\n",
    "bigram_freq = [(word, bigram_sum[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "bigram_df = pd.DataFrame(bigram_freq, columns=['Bigram', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Select top 20\n",
    "top_bigrams = bigram_df.head(20)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(y='Bigram', x='Frequency', data=top_bigrams, palette='coolwarm')\n",
    "plt.title('Top 20 Most Frequent Bigrams', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Bigrams')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cb1eb",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The most frequent bigram \"sxsw link\" followed by \"link sxsw\" then \"apple store\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79dda72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trigram vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3), stop_words='english')\n",
    "trigrams = vectorizer.fit_transform(dataset['clean_tweet'])\n",
    "\n",
    "# Sum up trigram occurrences\n",
    "trigram_sum = trigrams.sum(axis=0)\n",
    "trigram_freq = [(word, trigram_sum[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "trigram_df = pd.DataFrame(trigram_freq, columns=['Trigram', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Select top 20\n",
    "top_trigrams = trigram_df.head(20)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y='Trigram', x='Frequency', data=top_trigrams, palette='coolwarm')\n",
    "plt.title('Top 20 Most Frequent Trigrams', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Trigrams')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2fd16",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The most frequent Trigram in the dataset is 'new social network' closely followed by 'social network circle' and 'major new social'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec80f9",
   "metadata": {},
   "source": [
    "## Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same muted color palette from the pie chart\n",
    "colors = sns.color_palette('muted')\n",
    "\n",
    "# Plot sentiment distribution across brands\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(\n",
    "    data=dataset,\n",
    "    x='product_updated',\n",
    "    hue='sentiment_text',       # use text labels for clearer legend\n",
    "    palette=colors\n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Sentiment Distribution Across Brands\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Brand / Product\", fontsize=12)\n",
    "plt.ylabel(\"Tweet Count\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Sentiment\", loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe1d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate tweets by sentiment\n",
    "positive_tweets = ' '.join(dataset[dataset['clean_sentiment'] == 1]['clean_tweet'])\n",
    "negative_tweets = ' '.join(dataset[dataset['clean_sentiment'] == 0]['clean_tweet'])\n",
    "neutral_tweets  = ' '.join(dataset[dataset['clean_sentiment'] == 2]['clean_tweet'])\n",
    "\n",
    "# Generate Word Clouds\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18,6))\n",
    "\n",
    "for ax, text, title in zip(\n",
    "    axes,\n",
    "    [positive_tweets, negative_tweets, neutral_tweets],\n",
    "    ['Positive', 'Negative', 'Neutral']\n",
    "):\n",
    "    wc = WordCloud(width=600, height=400, background_color='white', colormap='viridis').generate(text)\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.set_title(f\"Top Words in {title} Tweets\", fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_brand = dataset.groupby(['product_updated', 'clean_sentiment']).size().unstack(fill_value=0)\n",
    "sentiment_brand.plot(kind='bar', stacked=True, figsize=(12,6), colormap='coolwarm')\n",
    "plt.title(\"Sentiment Composition by Brand/Product\")\n",
    "plt.xlabel(\"Brand/Product\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(['Positive', 'Negative', 'Neutral'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c04a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(\n",
    "    data=dataset,\n",
    "    x='sentiment_text',\n",
    "    y='tweet_length',\n",
    "    hue='product_updated',\n",
    "    palette='muted'   # or 'Set2', 'coolwarm', 'husl', etc.\n",
    ")\n",
    "plt.title(\"Tweet Length Distribution by Product and Sentiment\", fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ab672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Compute tweet length (number of words in each cleaned_tweet)\n",
    "dataset['tweet_length'] = dataset['clean_tweet'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Step 2: Define sentiment mapping (for labels)\n",
    "sentiment_map = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n",
    "dataset['sentiment_text'] = dataset['clean_sentiment'].map(sentiment_map)\n",
    "\n",
    "# Step 3: Define color palette consistent with previous plots\n",
    "custom_palette = {\n",
    "    'Positive': 'green',\n",
    "    'Negative': 'red',\n",
    "    'Neutral': 'gray'\n",
    "}\n",
    "\n",
    "# Step 4: Plot the boxplot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(\n",
    "    data=dataset,\n",
    "    x='sentiment_text',\n",
    "    y='tweet_length',\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "# Step 5: Customize appearance\n",
    "plt.title(\"Tweet Length vs Sentiment\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Tweet Length (Word Count)\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ad032",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "sns.boxplot(\n",
    "    data=dataset,\n",
    "    x='sentiment_text',\n",
    "    y='tweet_length',\n",
    "    hue='product_updated',\n",
    "    palette='Set3'  # automatically assigns distinct colors per product\n",
    ")\n",
    "plt.title(\"Tweet Length Distribution by Product and Sentiment\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Tweet Length (Word Count)\")\n",
    "plt.legend(title='Product', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc9ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
